---
title: "Lecture 3: Exploratory Data Analysis - Part 1"
subtitle: "Intro to Data Science for Public Policy, Spring 2016"
author: "by Jeff Chen & Dan Hammer, Georgetown University McCourt School of Public Policy"
output: 
  html_document: 
    theme: journal
    toc: yes
---


Mobile technologies have lowered the bar to using lightweight sensors that measure the physical world and have opened new applications of data in daily life. From a smart phone's accelerometer, it’s possible to track distinct patterns in one’s activity based on the fluctuations in acceleration ($\frac{m}{s^{2}}$). In fact, many of these technologies have become commonly available, enabling physical fitness activity monitoring to characterizing transportation quality. Below is a set of exercise measurements from an smartphone accelerometer that lasted approximately 6.5 minutes and graphed at a frequency of 5 hertz (five readings per second). 

Can you visually identify distinct patterns? What makes those patterns distinct?


```{r, echo=F, warning=FALSE, message=FALSE}
setwd("/Users/jeff/Documents/Github/data-science/lecture-03/example_data")
files <- list.files()[1]


#install.packages("dygraphs")
library(dygraphs)
library(xts)

for(i in files){
  temp <- read.csv(i)
  temp$accel <- sqrt(temp$user_acc_x.G.^2 + temp$user_acc_y.G.^2 + temp$user_acc_z.G.^2)
  temp <- temp[!is.na(temp$accel) & temp$accel!="",]
  #print(nrow(temp))

  temp$timestamp.unix. <- round(temp$timestamp.unix./0.2,0)*0.2
  temp <- temp[,c("timestamp.unix.","accel")]
  temp_val <- aggregate(x = temp$accel, by=list(temp$timestamp.unix.), FUN=mean, na.rm=TRUE)
  colnames(temp_val) <- c("time","accel")

  temp_val <- temp_val[(nrow(temp_val)-2300):nrow(temp_val),]
  temp_val$time <- temp_val$time - min(temp_val$time) 
  
  temp_val <- temp_val[order(temp_val$time),]
  temp_val[,1] <- as.POSIXct(as.numeric(as.character(temp_val[,1])),origin="2016-01-01")
  
  xts_temp <- xts(temp_val[,2], order.by=temp_val[,1])
  assign(gsub(".csv","",i),xts_temp)
}

dygraph(exercise,  group = "other", height=250) %>% dyLegend( show = "never") %>% dyAxis("y", label = "m/s^2", valueRange = c(0, 1)) 

```



Over the short time sample, the graphs indicate four distinct types of acceleration patterns. If we manually extract samples from these periods, we can quantify the patterns in terms of their central tendencies. Idle periods have near zero acceleration, walking periods have acceleration around 0.2 with tight dispersion, running periods hover around 0.6 +/- 0.2, and descending stairs vary widely. Given how these basic insights, we can experiment with various methods of *feature extraction*, or ways to distill and represent useful signal from the raw data. We can also begin to formulate hypotheses for how to model and represent the patterns and relationships in the data. 


```{r, echo=F, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)

idle <- temp_val[250:350,]
walk <- temp_val[45:175,]
run <- temp_val[1135:1200,]
stairs <- temp_val[1400:1700,]

p = ggplot(idle,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line() + ylim(0, 1) + ggtitle(paste("Idle: mu = ", round(mean(idle[,2]),2),", +/- = ",round(sd(idle[,2])*1.96,2),", max = ",round(max(idle[,2]),2))) + theme(plot.title = element_text(size = 10))

p1 = ggplot(walk,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line() + ylim(0, 1) + ggtitle(paste("Walk: mu = ", round(mean(walk[,2]),2),", +/- = ",round(sd(walk[,2])*1.96,2),", max = ",round(max(walk[,2]),2)))+ theme(plot.title = element_text(size = 10))

p2 = ggplot(run,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line() + ylim(0, 1) + ggtitle(paste("Run: mu = ", round(mean(run[,2]),2),", +/- = ",round(sd(run[,2])*1.96,2),", max = ",round(max(run[,2]),2)))+ theme(plot.title = element_text(size = 10))

p3 = ggplot(stairs,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line() + ylim(0, 1) + ggtitle(paste("Descend Stairs: mu = ", round(mean(stairs[,2]),2),", +/- = ",round(sd(stairs[,2])*1.96,2),", max = ",round(max(stairs[,2]),2)))+ theme(plot.title = element_text(size = 10))

grid.arrange(p,p1,p2,p3, ncol=2)
```


This is the basis of *exploratory data analysis* or *EDA* -- the first look at a dataset. Much of exploratory data analysis is focused on formulating hypotheses, assessing data structures, and understanding the quirks and tolerances of data in order to develop useful and insightful applications of the data. This is achieved by using visual techniques (e.g. histograms, other graphs) to identify outliers and assess the distribution of variables as well as statistical measures help to understand central tendency among other properties of the data. A well-conducted EDA would yield critical insights into how the data can and should be processed, methods for feature extraction to represent usable signal in the data, and options for operationalizing a strategy for solving the problem at hand. 

We can break EDA into a series of high-level goals, each of which is associated with analytical tasks that help piece together a clearer picture of what is contained in the data. Each of these questions can be answered through a graphical or numerical technical approach.


| **Goal**  | **Common Questions** |
|--------------------+-----------------------------------------------------------------------------------|
| Assess the data types  |  - Are the data categorical, numerical, factor, strings, other? <br> - What manipulations will you need to perform to get the data into usable shape?  |
| Understand the empirical distributions | - Does the data fall into a commonly recognized shape? <br> - Is it unimodal, bimodal? <br> - Is there any indication of time-dependence? |
| Detect outliers, missingness and errors | - Are there anomalous values? <br> - Do records spike or occur during odd times? <br> - How complete is the data? <br> - Which variables need to be standardized and cleaned?  | 
| Check the assumptions | - How exactly is the data collected? <br> - Does the data reflect what would be expected?|
| Identify important variables | -Which variables are correlated with one another?  |
| Formulate data-bounded hypotheses| - Which variables are most correlated?|

This chapter will equip you with the skills to explore the data in an efficient and thoughtful manner. We begin with an overview of three main issues that are investigated through EDA, then reinforce by walking through an example workflow.

<hr>


##Three things
While there are many approaches to EDA, there are three considerations that underlie effective discovery: data structure, statistical measures, and graphical summaries. 

###Structure
The structure of the data dictates the amount and type of processing required to make data usable and wieldy. Typically, data should be in matrix or tabular form, which allows for data to be manipulated and analyzed. However, at times, the raw data may be in an unstructured format (e.g. raw text) or in different structured formats (e.g. satellite imagery), requiring processing, reshaping and/or feature extraction so that the data is usable.

####Where to start
To start, we'll create a dummy data set, then run the `str()` method to produce a structural summary of each variable in a dataset.
```{r}

wide <- data.frame(person_id = 1:12, #Count from 1 to 12
                   sex = round(runif(12)),
                   age = as.character(rnorm(12,40,10)),
                   weight = sample(c(rep(-9999, 10),round(rnorm(12,120,30))),12),
                   program = rep(c("a","b"),6),
                   participation = rep(c("in","out",NA),4))


str(wide)
```

There are a number of key things to keep in mind, including:

- <u> Some numbers are formatted as factors rather than integers</u>. In particular, *age* will need to be converted by using a nested function `as.numeric(as.character(data$age))` as factors need to be converted to strings before being converted to numbers.
- <u> Some numbers are coded as numeric but represent discrete values in order to keep the file sizes smaller</u>. For example, *sex* is coded as a binary integer, but represents two values: 0 = Female, 1 = Male.
- <u> Check for missing values</u>. Missing values are often coded as `NA` or a large negative number in cases where values should be non-zero positive such as `-9999`. This will require some cleaning to standardize values. 

At 
While `str()` provides a fairly comprehensive overview, `colnames()` returns the column names and `dim()` returns the dimensions of the data.

```{r}

colnames(data)
dim(data)
```


###Statistical Measures
John Tukey, one of the great statisticians of the 20th Century, advocated for EDA as the first step in working with data and has since become standard practice. From his perspective,  *Numerical quantities focus on expected values, graphical summaries on unexpected values.* 

[Statistical]
summarize 
- check for central tendency
- spread or dispersion of data

####The usual suspects: quantitative variables
| Measure | Formula | Definition | R Function |Example |
|---------+-----------+---------------------+---------+-------------|
| Mean or Average | $$ \mu = \frac{1}{n} \sum_{i=1}^{n} x_i$$ | A measure of central tendency formulated as the sum of all values. Also known as the 'expected value'. | mean()| `r paste0("mean(c(1,3,5,7,9)) = ", mean(c(1,3,5,7,9)))`|
| Median |  Value at $$(\frac{n+1}{2})^{th} $$ position| A measure of central tendency based on the 'middle value' or 50th percentile of a random variable.  | median() | `r paste0("median(c(1,3,5,7,9)) = ", median(c(1,3,5,7,9)))`|
| Variance | $$ \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i-\mu)^2$$ | A measure of dispersion around the mean. | var() | `r paste0("var(c(1,3,5,7,9)) = ", var(c(1,3,5,7,9)))`|
| Standard Deviation | $$ \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i-\mu)^2}$$ | The square root of the variance. | sd() | `r paste0("sd(c(1,3,5,7,9)) = ", sd(c(1,3,5,7,9)))`|
| Minimum |  $$ \operatorname{argmin}(x_i)$$ | The smallest value of a random variable. | min()| `r paste0("min(c(1,3,5,7,9)) = ", min(c(1,3,5,7,9)))`|
| Maximum |  $$ \operatorname{argmax}(x_i)$$ | The largest value of a random variable.  | max()| `r paste0("max(c(1,3,5,7,9)) = ", max(c(1,3,5,7,9)))`|
| Range |  $$\operatorname{argmax}(x_i) - \operatorname{argmin}(x_i)$$ | Difference of its largest and smallest data values| range()| `r paste0("range(c(1,3,5,7,9)) = ", range(c(1,3,5,7,9)))`|
| Missing |  $$ x_i = NA $$ | Number of values that are NA. Note that in some cases, -9999 or similar representation may represent missingness.| sum(is.na()) | `r paste0("sum(is.na((c(1,3,5,7,9))) = ", sum(is.na((c(1,3,NA,5,7,9,NA)))))`|


####The usual suspects: discrete variables
| Measure | Formula | Definition | R Function |Example |
|---------+-----------+---------------------+---------+-------------|
| Mean or Average | $$ \mu = \frac{1}{n} \sum_{i=1}^{n} x_i$$ | A measure of central tendency formulated as the sum of all values. Also known as the 'expected value'. | mean()| `r paste0("mean(c(1,3,5,7,9)) = ", mean(c(1,3,5,7,9)))`|
| Median |  Value at $$(\frac{n+1}{2})^{th} $$ position| A measure of central tendency based on the 'middle value' or 50th percentile of a random variable.  | median() | `r paste0("median(c(1,3,5,7,9)) = ", median(c(1,3,5,7,9)))`|


###Graphical Approaches
[Text goes here]

####Univariate 
[Goals]

```{r, message=FALSE, warning=FALSE, eval=F}


#Graphs
base <- ggplot(acs, aes(AGEP)) 

#Histogram
m1 <- base + ggtitle("(a) Histogram")+  geom_histogram(colour = "white", fill = "navy") + labs(x = "Age")

#K-density
m2 <-  base + ggtitle("(b) Kernel Density")+ geom_density(colour = "white", fill = "navy") + labs(x = "Age")

#Violin Plot
m3 <- ggplot(acs, aes(factor(HICOV),AGEP))  + geom_violin(colour = "navy", fill = "navy")  + ggtitle("(c) Violin plot") + labs(x = "Age")

#Boxplot
m4 <- ggplot(acs, aes(factor(HICOV),AGEP)) + geom_boxplot(colour = "grey", fill = "navy")  + ggtitle("(d) Boxplot") + labs(x = "Age")

#Density overlap
m5 <- ggplot(acs, aes(AGEP, fill = factor(HICOV), colour = factor(HICOV))) +
  geom_density(alpha = 0.1) + labs(x = "Age") + ggtitle("(e) Kernel Overlay")

#Arrange graphs into 2 columns
grid.arrange(m1, m2, m3, m4, m5, layout_matrix = cbind(c(1, 4),c(2,5),c(3,5)), ncol=3)
```


####Thought exercise: Combining statistical thinking and graphical analysis
>You've been asked to take a look at load times for an internet search website. 
<br>- Which of the following three graphs best represents the distribution of latencies? <br>- Where would the *mean* be relative to the *median*?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2}
a = rnorm(100000,0,1)^2
b = rnorm(100000,7,2)
c = log(a)-min(log(a))

par(mfrow=c(1,3))
hist(a, probability=TRUE, xlab="Time (s)",main="Option #1", ylab="% Web Accesses", col="navy")
hist(b, probability=TRUE,  xlab="Time (s)",main="Option #2", ylab="% Web Accesses", col="navy")
hist(c, probability=TRUE, xlab="Time (s)",main="Option #3", ylab="% Web Accesses", col="navy")

```

####Bivariate Graphs
[Examine relationships]
```{r,  message=FALSE, warning=FALSE, eval=F}

##Scatter plot
p = ggplot(acs,aes(x=AGEP,y=log(PERNP)))  +
  xlab("Age") +
  ylab("log(earnings)")
p1 = p + geom_point(size=1) + ggtitle("(a) scatter")
p2 = p + geom_point(alpha = 0.1, colour="navy", size=1) +
  theme_bw() + ggtitle("(b) scatter (alpha = 0.1)")

##Hexbin 
p3 = p +
  stat_bin_hex(colour="white", na.rm=TRUE,alpha=0.9) +
  scale_fill_gradientn(colours=c("lightgrey","navy"), name = "Frequency", na.value=NA) + 
  guides(fill=FALSE) + ggtitle("(c) hex-bin plot")

#Scatter by Group
p4  = ggplot(acs,aes(x=AGEP,y=log(PERNP),colour=as.factor(HICOV)))+
  ggtitle("(d) scatter by group") + theme(legend.position="none") +
  geom_point(alpha = 0.3, size=1) +
  xlab("Age") + ylab("log(earnings)")

#Contour graph
p5  = p + ggtitle("(e) contour")+ 
  geom_density2d() + 
  theme_bw()

#Scatter plot with regression line (locally weighted smoother)
p6 = ggplot(acs, aes(x=AGEP, y=log(PERNP))) + ggtitle("(f) scatter + regression line")+
  geom_point(shape=1,alpha = 0.6, colour="navy",size=1) + 
  geom_smooth() + xlab("Age") + ylab("log(earnings)")

#Arrange graphs
grid.arrange(p1,p2,p3,p4,p5,p6, ncol=3)
 
```


###Homework 
- GHCN-M: Missing values analysis in matrix of weather anomalies from 1880 to Present

