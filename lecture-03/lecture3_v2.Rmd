---
title: "Lecture 3: Exploratory Data Analysis - Part 1"
subtitle: "Intro to Data Science for Public Policy, Spring 2016"
author: "by Jeff Chen & Dan Hammer, Georgetown University McCourt School of Public Policy"
output: 
  html_document: 
    theme: journal
    toc: yes
---
Novice data scientists all too often default to a familiar machine learning model and jump straight into building an application before thoroughly interrogating the data. By foregoing *exploratory data analysis* (EDA), analysts may skip perhaps the most formative step in the technical exercise of a data science question that forces one to ask meticulous contextual, quality-concerned questions that ensure that a data product is well-designed from a technical lens. 

###So, what is EDA?
Much of exploratory data analysis is visual in nature to provide an organic understanding of the shape and quality of information, capitalizing on the visual pattern detection that which human minds are adept.  In many respects, exploratory data analysis allows analysts to bound their problem by the limitation of the data, but by first identifying the bounds of the data through analysis. A practice that was promoted by statistician John Tukey, EDA has become a pillar of data disciplines.

> *Numerical quantities focus on expected values, graphical summaries on unexpected values.*
>   <br>     - John Tukey

The practice of EDA can be described by high-level goals associated with common questions. Each of these questions can be answered through a graphical or numerical technical approach.


| **Goal**  | **Common Questions** |
|--------------------+-----------------------------------------------------------------------------------|
| Assess the data types  |  - Are the data categorical, numerical, factor, strings, other? <br> - What manipulations will you need to perform to get the data into usable shape?  |
| Understand the empirical distributions | - Does the data fall into a commonly recognized shape? <br> - Is it unimodal, bimodal? <br> - Is there any indication of time-dependence? |
| Detect outliers, missingness and errors | - Are there anomalous values? <br> - Do records spike or occur during odd times? <br> - How complete is the data? <br> - Which variables need to be standardized and cleaned?  | 
| Check the assumptions | - How exactly is the data collected? <br> - Does the data reflect what would be expected?|
| Identify important variables | -Which variables are correlated with one another?  |
| Formulate data-bounded hypotheses| - Which variables are most correlated?|

###A Workflow
EDA typically involves the following three 

- Data structure
- Simple Statistics
- Graphical Approaches


###Data For Context
To put EDA into context, we will rely upon the American Community Survey ([ACS](http://www.census.gov/programs-surveys/acs/)), which is one of the most relied upon public data sources in the United States. A survey that is produced by the U.S. Census Bureau, the ACS provides a highly detailed socioeconomic snapshot of households and communities, allowing for data-driven insight to inform public policy as well as business decisions. For this lesson, we will examine

Note: While each record is associated with a sampling weight, we will treat each record with equal weight. We will also only focus on one state in this exercise -- in this case, we have selected Iowa. The same analysis can be replicated for other states.

#####Get data, unpack, read into dataframe named 'acs'
```{r, warning=FALSE, message=FALSE}
temp <- tempfile()
download.file("https://www2.census.gov/programs-surveys/acs/data/pums/2015/1-Year/csv_pia.zip",temp, mode="wb")
unz <- unzip(temp, exdir=getwd())
acs <- read.csv(unz[1])
```


###(1) Data Structure
```{r, eval=FALSE}
colnames(acs)
str(acs) 
```

###(2) Statistical Measures
[Statistical]
summarize 
- check for central tendency
- spread or dispersion of data

####The usual suspects: quantitative variables
| Measure | Formula | Definition | R Function |Example |
|---------+-----------+---------------------+---------+-------------|
| Mean or Average | $$ \mu = \frac{1}{n} \sum_{i=1}^{n} x_i$$ | A measure of central tendency formulated as the sum of all values. Also known as the 'expected value'. | mean()| `r paste0("mean(c(1,3,5,7,9)) = ", mean(c(1,3,5,7,9)))`|
| Median |  Value at $$(\frac{n+1}{2})^{th} $$ position| A measure of central tendency based on the 'middle value' or 50th percentile of a random variable.  | median() | `r paste0("median(c(1,3,5,7,9)) = ", median(c(1,3,5,7,9)))`|
| Variance | $$ \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i-\mu)^2$$ | A measure of dispersion around the mean. | var() | `r paste0("var(c(1,3,5,7,9)) = ", var(c(1,3,5,7,9)))`|
| Standard Deviation | $$ \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i-\mu)^2}$$ | The square root of the variance. | sd() | `r paste0("sd(c(1,3,5,7,9)) = ", sd(c(1,3,5,7,9)))`|
| Minimum |  $$ \operatorname{argmin}(x_i)$$ | The smallest value of a random variable. | min()| `r paste0("min(c(1,3,5,7,9)) = ", min(c(1,3,5,7,9)))`|
| Maximum |  $$ \operatorname{argmax}(x_i)$$ | The largest value of a random variable.  | max()| `r paste0("max(c(1,3,5,7,9)) = ", max(c(1,3,5,7,9)))`|
| Range |  $$\operatorname{argmax}(x_i) - \operatorname{argmin}(x_i)$$ | Difference of its largest and smallest data values| range()| `r paste0("range(c(1,3,5,7,9)) = ", range(c(1,3,5,7,9)))`|
| Missing |  $$ x_i = NA $$ | Number of values that are NA. Note that in some cases, -9999 or similar representation may represent missingness.| sum(is.na()) | `r paste0("sum(is.na((c(1,3,5,7,9))) = ", sum(is.na((c(1,3,NA,5,7,9,NA)))))`|


####The usual suspects: discrete variables
| Measure | Formula | Definition | R Function |Example |
|---------+-----------+---------------------+---------+-------------|
| Mean or Average | $$ \mu = \frac{1}{n} \sum_{i=1}^{n} x_i$$ | A measure of central tendency formulated as the sum of all values. Also known as the 'expected value'. | mean()| `r paste0("mean(c(1,3,5,7,9)) = ", mean(c(1,3,5,7,9)))`|
| Median |  Value at $$(\frac{n+1}{2})^{th} $$ position| A measure of central tendency based on the 'middle value' or 50th percentile of a random variable.  | median() | `r paste0("median(c(1,3,5,7,9)) = ", median(c(1,3,5,7,9)))`|


###(3) Graphical Approaches
[Text goes here]

####Univariate 
[Goals]
```{r, message=FALSE, warning=FALSE}

library(ggplot2)
library(gridExtra)

test <- acs[acs$PERNP<= mean(acs$PERNP, na.rm=T)+2*sd(acs$PERNP, na.rm=T),]

#Graphs
base <- ggplot(acs, aes(AGEP)) 

#Histogram
m1 <- base + ggtitle("(a) Histogram")+  geom_histogram(colour = "white", fill = "navy") + labs(x = "Age")

#K-density
m2 <-  base + ggtitle("(b) Kernel Density")+ geom_density(colour = "white", fill = "navy") + labs(x = "Age")

#Violin Plot
m3 <- ggplot(acs, aes(factor(HICOV),AGEP))  + geom_violin(colour = "navy", fill = "navy")  + ggtitle("(c) Violin plot") + labs(x = "Age")

#Boxplot
m4 <- ggplot(acs, aes(factor(HICOV),AGEP)) + geom_boxplot(colour = "grey", fill = "navy")  + ggtitle("(d) Boxplot") + labs(x = "Age")

#Density overlap
m5 <- ggplot(acs, aes(AGEP, fill = factor(HICOV), colour = factor(HICOV))) +
  geom_density(alpha = 0.1) + labs(x = "Age") + ggtitle("(e) Kernel Overlay")

#Arrange graphs into 2 columns
grid.arrange(m1, m2, m3, m4, m5, layout_matrix = cbind(c(1, 4),c(2,5),c(3,5)), ncol=3)
```


####Thought exercise: Combining statistical thinking and graphical analysis
>You've been asked to take a look at load times for an internet search website. 
<br>- Which of the following three graphs best represents the distribution of latencies? <br>- Where would the *mean* be relative to the *median*?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2}
a = rnorm(100000,0,1)^2
b = rnorm(100000,7,2)
c = log(a)-min(log(a))

par(mfrow=c(1,3))
hist(a, probability=TRUE, xlab="Time (s)",main="Option #1", ylab="% Web Accesses", col="navy")
hist(b, probability=TRUE,  xlab="Time (s)",main="Option #2", ylab="% Web Accesses", col="navy")
hist(c, probability=TRUE, xlab="Time (s)",main="Option #3", ylab="% Web Accesses", col="navy")

```

####Bivariate Graphs
[Examine relationships]
```{r,  message=FALSE, warning=FALSE}

##Scatter plot
p = ggplot(acs,aes(x=AGEP,y=log(PERNP)))  +
  xlab("Age") +
  ylab("log(earnings)")
p1 = p + geom_point(size=1) + ggtitle("(a) scatter")
p2 = p + geom_point(alpha = 0.1, colour="navy", size=1) +
  theme_bw() + ggtitle("(b) scatter (alpha = 0.1)")

##Hexbin 
p3 = p +
  stat_bin_hex(colour="white", na.rm=TRUE,alpha=0.9) +
  scale_fill_gradientn(colours=c("lightgrey","navy"), name = "Frequency", na.value=NA) + 
  guides(fill=FALSE) + ggtitle("(c) hex-bin plot")

#Scatter by Group
p4  = ggplot(acs,aes(x=AGEP,y=log(PERNP),colour=as.factor(HICOV)))+
  ggtitle("(d) scatter by group") + theme(legend.position="none") +
  geom_point(alpha = 0.3, size=1) +
  xlab("Age") + ylab("log(earnings)")

#Contour graph
p5  = p + ggtitle("(e) contour")+ 
  geom_density2d() + 
  theme_bw()

#Scatter plot with regression line (locally weighted smoother)
p6 = ggplot(acs, aes(x=AGEP, y=log(PERNP))) + ggtitle("(f) scatter + regression line")+
  geom_point(shape=1,alpha = 0.6, colour="navy",size=1) + 
  geom_smooth() + xlab("Age") + ylab("log(earnings)")

#Arrange graphs
grid.arrange(p1,p2,p3,p4,p5,p6, ncol=3)
 
```


###Exercise Data
- GHCN-M: Missing values analysis in matrix of weather anomalies from 1880 to Present