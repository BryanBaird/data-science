---
title: "Lecture 3: Exploratory Data Analysis - Part 1"
subtitle: "Intro to Data Science for Public Policy, Spring 2016"
author: "by Jeff Chen & Dan Hammer, Georgetown University McCourt School of Public Policy"
output: 
  html_document: 
    theme: journal
    toc: yes
---
Novice data scientists all too often default to a familiar machine learning model and jump straight into building an application before thoroughly interrogating the data. By foregoing *exploratory data analysis* (EDA), analysts may skip perhaps the most formative step in the technical exercise of a data science question that forces one to ask meticulous contextual, quality-concerned questions that ensure that a data product is well-designed from a technical lens. 

###So, what is EDA?
Much of exploratory data analysis is visual in nature to provide an organic understanding of the shape and quality of information, capitalizing on the visual pattern detection that which human minds are adept.  In many respects, exploratory data analysis allows analysts to bound their problem by the limitation of the data, but by first identifying the bounds of the data through analysis. A practice that was promoted by statistician John Tukey, EDA has become a pillar of data disciplines.

> *Numerical quantities focus on expected values, graphical summaries on unexpected values.*
>   <br>     - John Tukey

The practice of EDA can be described by high-level goals associated with common questions. Each of these questions can be answered through a graphical or numerical technical approach.


| **Goal**  | **Common Questions** |
|--------------------+-----------------------------------------------------------------------------------|
| Assess the data types  |  - Are the data categorical, numerical, factor, strings, other? <br> - What manipulations will you need to perform to get the data into usable shape?  |
| Understand the empirical distributions | - Does the data fall into a commonly recognized shape? <br> - Is it unimodal, bimodal? <br> - Is there any indication of time-dependence? |
| Detect outliers, missingness and errors | - Are there anomalous values? <br> - Do records spike or occur during odd times? <br> - How complete is the data? <br> - Which variables need to be standardized and cleaned?  | 
| Check the assumptions | - How exactly is the data collected? <br> - Does the data reflect what would be expected?|
| Identify important variables | -Which variables are correlated with one another?  |
| Formulate data-bounded hypotheses| - Which variables are most correlated?|

###Common Workflow
EDA typically involves the following three 

- Data structure
- Simple Statistics
- Graphical Approaches


###Example Data 
- American Community Survey

```{r}
##American Population Survey -- Iowa Popualtion survey, 2015

temp <- tempfile()
download.file("https://www2.census.gov/programs-surveys/acs/data/pums/2015/1-Year/csv_pia.zip",temp, mode="wb")
unz <- unzip(temp, exdir=getwd())
acs <- read.csv(unz[1])

```
- GHCN-M: Missing values analysis in matrix of weather anomalies from 1880 to Present

## (1) Data Structure
```{r}
colnames(acs)
str(acs) 
```


## (2) Statistical Measures
[Statistical]
summarize 
- check for central tendency
- spread or dispersion of data

###The usual suspects
| Measure | Formula | Definition | R Function |Example |
|---------+-----------+---------------------+---------+-------------|
| Mean or Average | $$ \mu = \frac{1}{n} \sum_{i=1}^{n} x_i$$ | A measure of central tendency formulated as the sum of all values. Also known as the 'expected value'. | mean()| `r paste0("mean(c(1,3,5,7,9)) = ", mean(c(1,3,5,7,9)))`|
| Median |  Value at $$(\frac{n+1}{2})^{th} $$ position| A measure of central tendency based on the 'middle value' or 50th percentile of a random variable.  | median() | `r paste0("median(c(1,3,5,7,9)) = ", median(c(1,3,5,7,9)))`|
| Variance | $$ \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i-\mu)^2$$ | A measure of dispersion around the mean. | var() | `r paste0("var(c(1,3,5,7,9)) = ", var(c(1,3,5,7,9)))`|
| Standard Deviation | $$ \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i-\mu)^2}$$ | The square root of the variance. | sd() | `r paste0("sd(c(1,3,5,7,9)) = ", sd(c(1,3,5,7,9)))`|
| Minimum |  $$ \operatorname{argmin}(x_i)$$ | The smallest value of a random variable. | min()| `r paste0("min(c(1,3,5,7,9)) = ", min(c(1,3,5,7,9)))`|
| Maximum |  $$ \operatorname{argmax}(x_i)$$ | The largest value of a random variable.  | max()| `r paste0("max(c(1,3,5,7,9)) = ", max(c(1,3,5,7,9)))`|
| Range |  $$\operatorname{argmax}(x_i) - \operatorname{argmin}(x_i)$$ | Difference of its largest and smallest data values| range()| `r paste0("range(c(1,3,5,7,9)) = ", range(c(1,3,5,7,9)))`|
| Missing |  $$ x_i = NA $$ | Number of values that are NA. Note that in some cases, -9999 or similar representation may represent missingness.| sum(is.na()) | `r paste0("sum(is.na((c(1,3,5,7,9))) = ", sum(is.na((c(1,3,NA,5,7,9,NA)))))`|





## (3) Graphical Approaches
[Text goes here]
```{r, echo=FALSE}


library(ggplot2)
library(gridExtra)
datagen <- function(n, mu, sd, p_list){
  x1    <- rnorm(n, mu, sd) #master variable
  df <- data.frame(x1)
  
  for(p in p_list){
    
    #Set up parameters
    theta <- acos(p)  # corr angle
    x2    <- rnorm(n, mu, sd)  # create a random dataset
    ds     <- cbind(x1, x2) 
    cent  <- scale(ds, center=TRUE, scale=FALSE)   # centered columns (mean 0)
    
    Imat   <- diag(n)                               # identity matrix
    Q    <- qr.Q(qr(cent[ , 1, drop=FALSE]))      # QR-decomposition, just matrix Q
    P    <- tcrossprod(Q)          # = Q Q'       # projection onto space defined by x1
    x2o  <- (Imat - P) %*% cent[ , 2]                 # x2ctr made orthogonal to x1ctr
    Xc2  <- cbind(cent[ , 1], x2o)                # bind to matrix
    Y    <- Xc2 %*% diag(1/sqrt(colSums(Xc2^2)))  # scale columns to length 1
    
    x <- Y[ , 2] + (1 / tan(theta)) * Y[ , 1]     # new correlated vec
    cor(x1, x)  
    df <- cbind(df,x) #
  }
  colnames(df)<- c("x",paste0("x",p_list))
  return(df)
}
```

###Univariate 
```{r, message=FALSE, warning=FALSE}
#Create simulated data
  df <- datagen(1000, 0, 50, c(0.1,0.5,0.7,0.9))
#Rename variables
  colnames(df)<-c("y","x1","x2","x3","x4")
#Groups
  df$group <- 0
  df$group[df$x3>0.05] <-1

#Graphs
  base <- ggplot(df, aes(y)) 
  
#Histogram
  m1 <- base + ggtitle("(a) Histogram")+  geom_histogram(colour = "white", fill = "navy")
  
#K-density
  m2 <-  base + ggtitle("(b) Kernel Density")+ geom_density(colour = "white", fill = "navy")
  
#Violin Plot
  m3 <- ggplot(df, aes(factor(group),y))  + geom_violin(colour = "navy", fill = "navy")  + ggtitle("(c) Violin plot")

#Boxplot
  m4 <- ggplot(df, aes(factor(group),y)) + geom_boxplot(colour = "navy", fill = "navy")  + ggtitle("(d) Boxplot")

#Arrange graphs into 2 columns
  grid.arrange(m1, m2, m3, m4, ncol=2)
```


###Thought exercise
>You've been asked to take a look at load times for an internet search website. 
> - Which of the following three graphs best represents the distribution of latencies? 
> - Where would the *mean* be relative to the *median*?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2}
a = rnorm(100000,0,1)^2
b = rnorm(100000,7,2)
c = log(a)-min(log(a))

par(mfrow=c(1,3))
hist(a, probability=TRUE, xlab="Time (s)",main="Option #1", ylab="% Web Accesses", col="navy")
hist(b, probability=TRUE,  xlab="Time (s)",main="Option #2", ylab="% Web Accesses", col="navy")
hist(c, probability=TRUE, xlab="Time (s)",main="Option #3", ylab="% Web Accesses", col="navy")

```

###Bivariate Graphs
```{r,  message=FALSE, warning=FALSE}

##Scatter plot
p = ggplot(df,aes(x=x4,y=y))  +
  xlab("x4") +
  ylab("y")
p1 = p + geom_point() + ggtitle("(a) scatter")
p2 = p + geom_point(alpha = 0.1, colour="navy") +
  theme_bw() + ggtitle("(b) scatter (alpha = 0.1)")

##Hexbin 
p3 = p +
  stat_bin_hex(colour="white", na.rm=TRUE,alpha=0.9) +
  scale_fill_gradientn(colours=c("lightgrey","navy"), name = "Frequency", na.value=NA) + 
  guides(fill=FALSE) + ggtitle("(c) Hex bin plot")

#Scatter by Group
p4  = ggplot(df,aes(x=x4,y=y,colour=as.factor(round(x2*10,1)*20)))+
      ggtitle("(d) scatter by group") + theme(legend.position="none") +
      geom_point(alpha = 0.3)

#Contour graph
p5  = p + ggtitle("(e) contour")+ 
      geom_density2d() + 
      theme_bw()

#Scatter plot with regression line (locally weighted smoother)
p6 = ggplot(df, aes(x=x4, y=y)) + ggtitle("(f) scatter + regression line")+
    geom_point(shape=1,alpha = 0.6, colour="navy") + 
    geom_smooth() 

#Arrange graphs
  grid.arrange(p1,p2,p3,p4,p5,p6, ncol=3)
 
```
