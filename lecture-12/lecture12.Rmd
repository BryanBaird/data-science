---
title: "Lecture 12: Spatial Data and Mapping"
subtitle: "Intro to Data Science for Public Policy, Spring 2016"
author: "by Jeff Chen & Dan Hammer, Georgetown University McCourt School of Public Policy"
output: 
  html_document: 
    theme: journal
    toc: yes
---

The primary purpose of this section is to introduce basic spatial analysis, visualization, and APIs in `R`.  The secondary purpose is to examine whether state-level policy has an appreciable impact on the composition of farmers' markets in the Southwest.  We pull [a data set from data.gov](https://explore.data.gov/d/wfna-38ey) on 7,863 farmers' markets in the United States. Save the file as `farmers-mkts.csv` in the same directory as your analysis script.

```{r comment=NA, cache=TRUE, message=FALSE}
data <- read.csv("farmers-mkts.csv", header = TRUE)
head(data[c("State", "x", "y", "Herbs", "Meat")])
```

The only difference between this data set and the rectangular data sets we've dealt with in previous sections is that each record is tied to a specific place on earth.  The column `x` represents the farmers' market's longitude, and `y` represents the market's latitude.  Each record can therefore be represented as a point on a map.  There are two basic data models in geographic information systems (GIS):

1. **Vector**.  A representation of the world using points, lines, and polygons. Vector models are useful for storing data that has discrete boundaries, such as country borders, land parcels, and streets.
2. **Raster**. A representation of the world as a surface divided into a regular grid of cells. Raster models are useful for storing data that varies continuously across space, like an aerial photograph, a satellite image, a surface of chemical concentrations, or an elevation surface.

These are stored in multiple data formats, which have accrued over time based on GIS software providers.  For example, for vector, shapefiles (`.shp`) were invented and used by ESRI, whereas keyhold markup language (`.kml`) files were invented and used by Keyhole (which was acquired by Google and turned into Google Maps).  Raster formats may be even more diverse.  However, `R` has extension libraries to deal with most of these formats now.  For vector data, we will rely on open formats, and mainly [GeoJSON](http://geojson.org/).  This is an extension of the basic [JSON](http://www.json.org/) data format to handle geographic information.  It is the direction GIS is headed.

Ignore all that for a moment.  Use the [`maps`](https://cran.r-project.org/web/packages/maps/index.html) package to use map data that is already in `R` formats.  Note that there is another package, [`mapproj`](https://cran.r-project.org/web/packages/mapproj/index.html), that deals with reprojection.  There are many standards to convert a 3D globe into a 2D map, called projections.  We ignore this, too, for the time being, and rely on the sensible projection built into the [`maps`](https://cran.r-project.org/web/packages/maps/index.html) package.

```{r comment=NA, cache=TRUE, message=FALSE}
library(maps)
map("state", interior = FALSE)
map("state", boundary = FALSE, col = "gray", add = TRUE)
points(data$x, data$y, cex = 0.2, col = "blue")
```

Now consider only the farmers' markets in Colorado, Utah, New Mexico,
and Arizona.  There are 353 farmers' markets in these four states. (One
is mislabeled, and is actually in Pennsylvania.  Knock this out by limiting the farmers' markets based on longitude.)

```{r comment=NA, cache=TRUE, message=FALSE}
statelist <- c("New Mexico", "Colorado", "Arizona", "Utah")
state.data <- data[data$State %in% statelist, ]
state.data <- state.data[state.data$x < -80,]
dim(state.data)
```

Each column of the `state.data` frame contains information on a different feature of the market.  The last 24 columns are binary variables with entries `"Y"` or `"N"`, indicating whether the market sells cheese, for example, or accepts credit cards.  Is it possible to predict the location of farmers' markets, based purely on these features?  If so, then there may be something about state policy that has an observable, immediate impact on the composition of the markets.  Clean up the features, assigning a numerical indicator to the `"Y"` response:

```{r comment=NA, cache=TRUE, message=FALSE}
X <- state.data[8:ncol(state.data)]
X <- apply(X, 2, function(col) { ifelse(col == "Y", 1, 0) })
colnames(X)
```

Note that all variables in the feature matrix `X` are binary. Create a distance matrix between the observations in `X` using the binary method to measure distance between markets, where "distance" is not geographical distance, but a measure of how similar one market from every other.  We use the `"binary"` method to calculate distance, where the vectors are regarded as binary bits, so non-zero elements are *on* and zero elements are *off*. The distance is the proportion of bits in which only one is on amongst those in which at least one is on.

```{r comment=NA, cache=TRUE, message=FALSE}
dist.mat <- dist(X, method = "binary")
```

The `dist.mat` object is the basis for the hierarchical clustering algorithm in `R`, which sorts the markets to minimize distance between all elements.  Build and plot the tree.

```{r comment=NA, cache=TRUE, message=FALSE}
hclust.res <- hclust(dist.mat)
cl <- cutree(hclust.res, k=4)
plot(cut(as.dendrogram(hclust.res), h=0)$upper, leaflab = "none")
```

```{r comment=NA, cache=TRUE, message=FALSE}
coords <- state.data[ , c("x", "y")]

assignColor <- function(cl.idx) {
	col.codes <- c("#FF8000", "#0080FF", "#FFBF00", "#FF4000")
	return(col.codes[cl.idx])
}

map("state", interior=FALSE, xlim=c(-117, -101), ylim=c(28, 43))
map("state", boundary=FALSE, col="gray", add=TRUE, xlim=c(-117, -101), ylim=c(28, 43))
points(coords[["x"]], coords[["y"]], cex=1, pch=20, col=assignColor(cl))
```

It seems clear from these figures that farmers' markets in New Mexico
are distinctive from those in neighboring states, somehow.  We can
force the analysis into a traditional-ish regression discontinuity
test.  First, calculate the distance of each market to the New Mexico
border between Arizona and Colorado.  I have plugged in the three
points to define `segment`, the upside-down and backwards L-shaped
border.

```{r comment=NA, cache=TRUE, message=FALSE}
library(maptools)

.segDistance <- function(coord) {
	segment <<- cbind(
					c(-109.047546, -109.047546, -103.002319),
					c(31.33487100, 36.99816600, 36.99816600)
				)
	near.obj <- nearestPointOnSegment(segment, coord)
	return(as.numeric(near.obj[["distance"]]))
}
```

Note the use of `<<-`. The expressly local function =.segDistance= will return the distance between the supplied coordinate to the global line segment.  Apply this function to all coordinates.  The resulting object `dist` represents distance to the New Mexico border; and to indicate the side of the border, scale the distance for each market /within/ New Mexico by $-1$.  A distance of zero indicates the border itself.  This is beginning to look more and more like the regression discontinuity design, with the discontinuity at zero distance.

```{r comment=NA, cache=TRUE, message=FALSE}
dist <- apply(coords, 1, FUN=.segDistance)
dist <- dist * ifelse(state.data[["State"]] == "New Mexico", -1, 1)
```

Now, plot the predicted cluster with respect to distance from border.
Figure  and indicates a clear discontinuity at the
border.  Note, however, that the regression discontinuity analysis
that we learn is generally for functions, not correspondences.

```{r comment=NA, cache=TRUE, message=FALSE}
sel.cl <- cl <= 4
plot(dist[sel.cl], cl[sel.cl], pch = 20, col = "blue",
   xlab = "Distance to New Mexico border (in degrees)",
   ylab = "Cluster category", yaxt = "n")
abline(v = 0, lty = 3, col = "red")
axis(2, at = 1:4)
```

The plot in Figure \ref{fig:disc} is not hemicontinuous, indicating some discontinuity.  All the figures combined offer reasonably strong evidence that the New Mexico border significantly alters the composition of the markets.

Switch gears. Suppose, now, that we want to find the elevation of each of market in the four sample states.  For this, we can use the Google Elevation API, which relies on URL requests, like we've seen in previous sections.  The following two functions build the URL request for a collection of coordinates.

```{r comment=NA, cache=TRUE, message=FALSE}
convertCoords <- function(coord.collection) {
	apply(coord.collection, 1, function(x) { paste(x[2], x[1], sep = ",") })
}
  
library(rjson)
library(RCurl)
getElevation <- function(coord.collection) {
	base.url <- "http://maps.googleapis.com/maps/api/elevation/json?locations="
	params <- "&sensor=false"
	coord.str <- paste(convertCoords(coord.collection), collapse = "|")
	query <- paste(base.url, coord.str, params, sep="")
	gotten <- getURL(query)

	output <- fromJSON(gotten, unexpected.escape = "skip")$results

	.elev <- function(x) {
		return(x[1][["elevation"]])
	}

	res <- as.matrix(lapply(output, .elev))
	return(res)
}
```

The Google API does not accept URLs that are too long.  I am not sure
what qualifies as too long, but the 353 farmers' market coordinates
throw an error.  So, we'll partition the coordinate collection.

```{r comment=NA, cache=TRUE, message=FALSE}
partition <- function(df, each = 10) {
	s <- seq(ceiling(nrow(df) / each))
	res <- split(df, rep(s, each = each))
	return(res)
}

elev.split <- lapply(partition(coords), getElevation)
elevation <- unlist(elev.split)

mkts <- data.frame(
	x=coords[["x"]], 
	y=coords[["y"]],
	cluster=cl,
	elevation=elevation
)

head(mkts)
```

Applying the `getElevation` function to each partition will send out
multiple requests.  The `elevation` collection contains the elevation
for all farmers' markets.  This is pretty cool.  We don't need to
store the elevations on disk.  We can rely on Google's data and raster
sampling to grab the elevations on demand. 

You can, along with your co-authors, explore the spatial data in an
open and collaborative way.  Elevation, then, may be a good
cofactor to use in the regression discontinuity analysis.  Next time.

